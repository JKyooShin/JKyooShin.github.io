---
title: "TALL: Temporal Activity Localization via Language Query"
date: 2020-07-20 08:26:28 -0400
---

## TALL: Temporal Activity Localization via Language Query

### 문제 정의
Moment localization via language query 라는 영역을 2017년에 처음 제시한 논문이다. (같은 학회에서 Hendricks et al. Localizing Moments in Video with Natural Language 또한 거의 비슷한 영역을 제시하였..으나..) 
기존 Action Localization의 문제는  action을 미리 정의된 범주로 분류하기 때문에 행동을 이해하는데 있어서 제한이 있다는 점이다. 그렇기에 영상의 내용을 사람처럼 제대로 이해하기 위해선 자연어로 표현된 다양한 사물들의 관계와 행동을 학습해야한다. 이러한 방식의 학습을 위해 자연어로 정의된 문장에 가장 잘 상응하는 영상 내부 영역을 찾는 문제를 제시하였다. 
1, 문장을 기계가 이해해야하고

이 문제를 위해 Sliding Window 방식으로 영역을 이해하는 Cross-modal Temporal Regression Localizer (CTRL) 을 제시하였다. 마지막으로, 기존에 action localizaion을 위한 dataset인 Charades에 이 Task의 형태에 맞는  annotation을 덧붙힌 Charades-STA dataset을 제시하였다.

###  Model
이 논문에서 제시한 Model 의 이름은 Cross-modal Temporal Regression Localizer 이다. 
이 모델이 영상을 이해하는 방식은 Sliding window 방식으로, 영상을 특정 길이로 나눠서 한번에 한 구간만을 보는 방법이다. 이런 방식은 특정 길이 이후 / 이전의 정보를 얻을 수 없기에, 조금이나마 보완하고자 한 구간을 볼 때 그 이후 및 이전 정보도 같이 넣어주어 학습한다.
t 시점의 영상 구간 정보와 문장의 정보를 맞춰주기 위해서 t-1, t+1 시점의 영상정보를 Concatenate 한 video feature의 차원을 Fully Connected layer을 통해 문장의 차원과 동일하게 맞춰준다.
이 논문은 3가지 방법으로 두 다른 도메인의 정보를 섞는다.
1. 문장 정보와 비디오 정보의 단순 합 (Element wise -  Add)
2. 문장 정보와 비디오 정보의 단순 곱(Element wise multiplication)
3. 문장 정보와 비디오 정보를 이은 뒤 (Concatenate) 2배가 된 차원을 원래 차원으로 축소하기 위한 FC층
이 3가지 방법을 각기 독립적으로 진행 후, 결과를 모두 잇는다(concatenate).

이후 Fully-Connected  layer을 통해 2가지의 Output을 낸다.
1. 모델이 보고 있는 영상 내 구간이 문장과 일치할 확률을 의미하는 Alignment Score
2. 더 자세하게 학습하기 위해 모델이 받은 구간 내부에서 문장과 일치하는 시작점과 끝점을 계산하는 location regressor

예시로 20초짜리의 영상과 "남자가 밖을 보기 위해 창가로 달려갔다" 라는 문장이 있다. 대부분의 video understanding 영역이 쓰듯, 영상은 C3D 혹은 VGG, I3D등의 CNN 기반 Pretrained model을 통해 프레임 형태에서 특징만 추출된 벡터 형태로 바뀌게 된다. 이 벡터의 형태는 $$V \in  \mathbb{R}^{t*d}$$ 로 나타낼 수 있다. 여기서 t는 동영상의 시간, d는 pretrained model 의 차원을 의미하며 보통 4096차원을 많이 쓴다.
Sliding window의 방식을 위해 이 벡터를 특정 길이의 구간들로 조각낸다. 만약 20초짜리 영상을 5개의 구간으로 나눈다고 하면 4초짜리 구간 5개가 생기게 된다.  한번에 한개의 구간만을 보며, 문장과 얼마나 상응하는지를 계산하는 것이 Sliding window 방식이다.
CTRL 에서는 이 구간을 하나만 쓰지 않고 전, 후 구간의 정보를 합쳤기 때문에
$$ F_v =  ( (v_{1}  \oplus v_{2} \oplus v_{3}) , \cdots,  (v_{t-2} \oplus v_{t-1} \oplus  v_{t} )), f_v^t \in F_v , f_v^t \in  \mathbb{R}^{3t*d}$$
이런 식으로 표현할 수 있다.  Pooling 을 통해 3t의 차원을 압축해주고, 남은 d를 문장과 맞추기 위해 FC layer에 넣어준다.

다양한 길이의 단어를 갖고 있는 문장의 의미를 이해하기 위해서, 각 단어들을 스텐포드 대학에서 제공한 Glove 300d을 통해 각 단어의 특징을 추출한 벡터로 바꿔준다. 이후 순환 신경망 구조인 Skip Thoughts 혹은 LSTM을 사용하여 단어를 순서대로 넣어주고, 들어간 단어들의 벡터 크기와 변화량을 통해 문장의 의미를 학습한다. 마지막 단어를 넣은 후 나오는 결과를 문장을 대표하는 결과, sentence embedding으로 사용한다.
Sentence embedding과 Video feature의 크기를 맞춰준 후, 두 벡터를 섞어서 합쳐주고 output을 계산한다.

Alignment 값은 해당 구간이 정답인 Ground Truth와 얼마나 맞는지, 즉 Intersection of Union(IoU)에 대한 값을 기반으로 Cross entropy loss값을 계산한다.
Location Regressor 은 그 구역 내부의 시작, 끝 값을 구하는데 smooth L1 loss 값을 계산한다. 

결국 가장 높은 Alignment 값을 가진 구간의 Start와 End를 가장 잘 맞는 구간으로 도출하는 모델이다. 

두 Loss 를 합산 후 가장 적은 loss값을 갖는 가중치들을 구하며 학습이 진행된다.

### Charades-STA
당시에는 이런 문제를 해결하기 위한 dataset이 부족해서, Action recognition 을 위해 만들어진 dataset인 Charades를 기반으로, 긴 Action label들을 분해한 후 다양한 구조의 문장을 재조합함으로 Annotation을 만들었다고 한다.

###  Results 
초기 연구라 그렇게 기대할만한 결과는 확인할 수 없지만, C3D 가 VGG, LRCN보다 성능이 높은 것을 볼 수 있었으며 Skip-thought이 LSTM보다 더 좋은 성능을 보였다.

아래는 TaCos와 Charades-STA에 대한 성능이다.



\end{document}
